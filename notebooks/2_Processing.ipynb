{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2   # Change to %autoreload when development phase is over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a Javascript section of code for building the Jupyter notebook's table of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import libraries needed for setting parameters of operating system \n",
    "import os\n",
    "import sys\n",
    "## Import library for temporary files creation \n",
    "import tempfile \n",
    "## Import library for time management \n",
    "import time\n",
    "## Import multiprocessing and functools libraries\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import Pandas library\n",
    "import pandas as pd\n",
    "## Import Numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Import Matplotlib \n",
    "import matplotlib as mpl \n",
    "## agg backend is used to create plot as a .png file\n",
    "mpl.use('agg')\n",
    "## Import Matplotlib.pyplot for creating graphs\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add folder with SCR provided belong to this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add local module to the path\n",
    "src = os.path.abspath('../SRC')\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Environment variables when working on Linux Mint **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import environ_variables as envi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDMSESSION = mate \t\n",
      "MANDATORY_PATH = /usr/share/gconf/mate.mandatory.path \t\n",
      "MATE_DESKTOP_SESSION_ID = this-is-deprecated \t\n",
      "LESSOPEN = | /usr/bin/lesspipe %s \t\n",
      "MDM_LANG = fr_BE.UTF-8 \t\n",
      "LOGNAME = tais \t\n",
      "USER = tais \t\n",
      "HOME = /home/tais \t\n",
      "XDG_VTNR = 9 \t\n",
      "PATH = /usr/local/bin:/home/tais/BIN:/home/tais/bin:/home/tais/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/bin:/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/script:/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/lib \t\n",
      "CLICOLOR = 1 \t\n",
      "DISPLAY = :0.0 \t\n",
      "SSH_AGENT_PID = 25466 \t\n",
      "LANG = fr_BE.UTF-8 \t\n",
      "TERM = xterm-color \t\n",
      "SHELL = /bin/bash \t\n",
      "GIS_LOCK = $$ \t\n",
      "XAUTHORITY = /home/tais/.Xauthority \t\n",
      "SESSION_MANAGER = local/tais-HP-Z620-Workstation:@/tmp/.ICE-unix/25383,unix/tais-HP-Z620-Workstation:/tmp/.ICE-unix/25383 \t\n",
      "SHLVL = 1 \t\n",
      "QT_LINUX_ACCESSIBILITY_ALWAYS_ON = 1 \t\n",
      "INSIDE_CAJA_PYTHON =  \t\n",
      "QT_ACCESSIBILITY = 1 \t\n",
      "LD_LIBRARY_PATH = :/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/lib \t\n",
      "COMPIZ_CONFIG_PROFILE = mate \t\n",
      "WINDOWPATH = 9 \t\n",
      "GTK_OVERLAY_SCROLLING = 0 \t\n",
      "PYTHONPATH = :/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/etc/python:/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/etc/python/grass:/home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu/etc/python/grass/script \t\n",
      "GISBASE = /home/tais/SRC/GRASS/grass_trunk/dist.x86_64-pc-linux-gnu \t\n",
      "CLUTTER_BACKEND = x11 \t\n",
      "USERNAME = tais \t\n",
      "XDG_SESSION_DESKTOP = mate \t\n",
      "GDM_XSERVER_LOCATION = local \t\n",
      "XDG_RUNTIME_DIR = /run/user/1000 \t\n",
      "JPY_PARENT_PID = 11635 \t\n",
      "QT_STYLE_OVERRIDE = gtk \t\n",
      "SSH_AUTH_SOCK = /run/user/1000/keyring/ssh \t\n",
      "VTE_VERSION = 4205 \t\n",
      "GDMSESSION = mate \t\n",
      "GISRC = /home/tais/.grass7/rc \t\n",
      "GIT_PAGER = cat \t\n",
      "XDG_CONFIG_DIRS = /etc/xdg/xdg-mate:/etc/xdg \t\n",
      "XDG_CURRENT_DESKTOP = MATE \t\n",
      "XDG_SESSION_ID = c22 \t\n",
      "DBUS_SESSION_BUS_ADDRESS = unix:abstract=/tmp/dbus-FzQKwSsyzM,guid=573684252e16c96c145a408a5be69c31 \t\n",
      "_ = /usr/local/bin/jupyter \t\n",
      "XDG_SESSION_COOKIE = 8441891e86e24d76b9616edf516d5734-1541839920.911260-2123598289 \t\n",
      "DESKTOP_SESSION = mate \t\n",
      "WINDOWID = 83886086 \t\n",
      "LESSCLOSE = /usr/bin/lesspipe %s %s \t\n",
      "DEFAULTS_PATH = /usr/share/gconf/mate.default.path \t\n",
      "MPLBACKEND = module://ipykernel.pylab.backend_inline \t\n",
      "MDM_XSERVER_LOCATION = local \t\n",
      "GTK_MODULES = gail:atk-bridge \t\n",
      "XDG_DATA_DIRS = /usr/share/mate:/usr/local/share/:/usr/share/:/usr/share/mdm/ \t\n",
      "PWD = /media/tais/data/Dropbox/ULB/MAUPP/Traitements/population_modeling/notebooks \t\n",
      "COLORTERM = mate-terminal \t\n",
      "PYTHONLIB = /usr/lib/python2.7 \t\n",
      "LS_COLORS = rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36: \t\n",
      "PAGER = cat \t\n",
      "XDG_SEAT = seat0 \t\n"
     ]
    }
   ],
   "source": [
    "# Set environmental variables\n",
    "envi.setup_environmental_variables() \n",
    "# Display current environment variables of your computer\n",
    "envi.print_environmental_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** GRASS GIS Python libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import libraries needed to launch GRASS GIS in the jupyter notebook\n",
    "import grass.script.setup as gsetup\n",
    "## Import libraries needed to call GRASS using Python\n",
    "import grass.script as gscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Functions defined by the user **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that check existance and create GRASS GIS database folder if needed\n",
    "from grass_database import check_gisdb\n",
    "from grass_database import check_location\n",
    "from grass_database import check_mapset\n",
    "from grass_database import working_mapset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import functions for processing time information\n",
    "from processing_time import start_processing\n",
    "from processing_time import print_processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that generate a random name in the GRASS GIS environement\n",
    "from random_layer_name import random_layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that create color file for raster\n",
    "from colorise_raster import create_color_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that clip multiple raster according to extention of a vector layer\n",
    "from create_clumped_grid import create_clumped_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here after:\n",
    "- Enter the path to the directory you want to use as \"[GRASSDATA](https://grass.osgeo.org/programming7/loc_struct.png)\". \n",
    "- Enter the name of the location in which you want to work and its projection information in [EPSG code](http://spatialreference.org/ref/epsg/) format. Please note that the GRASSDATA folder and locations will be automatically created if not existing yet. If the location name already exists, the projection information will not be used.  \n",
    "- Enter the name you want for the mapsets which will be used later for Unsupervised Segmentation Parameter Optimization (USPO), Segmentation and Classification steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define a empty dictionnary for saving user inputs\n",
    "user={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Enter the path to GRASSDATA folder\n",
    "user[\"gisdb\"] = \"/media/tais/My_Book_1/MAUPP/Traitement/Population_modelling_dasymetry/GRASSDATA\"\n",
    "## Enter the name of the location (existing or for a new one)\n",
    "user[\"location\"] = \"Dakar_32628\"\n",
    "## Enter the EPSG code for this location \n",
    "user[\"locationepsg\"] = \"32628\"\n",
    "## Enter the name of the permanent mapset\n",
    "user[\"permanent_mapset\"] = \"PERMANENT\"\n",
    "## Enter the name of a working mapset\n",
    "user[\"dasym_mapset\"] = \"DASYMETRY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Check for existance of GRASSDATA folder, location and mapsets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here after, the python script will check if GRASSDATA folder, locations and mapsets already exist. If not, they will be automatically created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRASSDATA folder already exist\n"
     ]
    }
   ],
   "source": [
    "# Check if the GRASS GIS database exists and create it if not\n",
    "check_gisdb(user[\"gisdb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Dakar_32628 already exist\n"
     ]
    }
   ],
   "source": [
    "# Check if the location exists and create it if not, with the CRS defined by the epsg code \n",
    "check_location(user[\"gisdb\"],user[\"location\"],user[\"locationepsg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PERMANENT' mapset already exists in location 'Dakar_32628'\n"
     ]
    }
   ],
   "source": [
    "# Check if the mapset exists and create it if not\n",
    "check_mapset(user[\"gisdb\"],user[\"location\"],user[\"permanent_mapset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DASYMETRY' mapset already exists in location 'Dakar_32628'\n"
     ]
    }
   ],
   "source": [
    "# Check if the mapset exists and create it if not\n",
    "check_mapset(user[\"gisdb\"],user[\"location\"],user[\"dasym_mapset\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch GRASS GIS working session on DASYMETRY mapset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are now working in mapset 'DASYMETRY'\n"
     ]
    }
   ],
   "source": [
    "# Change the current working GRASS GIS session mapset\n",
    "working_mapset(user[\"gisdb\"],user[\"location\"],user[\"dasym_mapset\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the name of main layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the name of the layers\n",
    "Land_cover = 'landcover'  # VHR Land cover map\n",
    "Land_use = 'landuse' # VHR Land use map\n",
    "mr_builtup = 'MR_builtup' # MR built-up map\n",
    "mr_built_pixelvalue = '1'\n",
    "grid = 'clumped_grid'\n",
    "## Name of the column containing the population count\n",
    "population_column=\"POPULATION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the resolution of the prediction grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tile_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set several variables and parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare list that will contain the name/paths of temporary layers/files\n",
    "TMP_MAPS = []\n",
    "TMP_CSV = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare strings that will contain the log of the processing\n",
    "log_text = \"\"\n",
    "log_text_extend = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of job that could be runned in parallel\n",
    "n_jobs = 15\n",
    "# Check if the computer has enough cores\n",
    "if(n_jobs >= multiprocessing.cpu_count()):\n",
    "    gscript.fatal(_(\"Requested number of jobs is > or = to available ressources. \\\n",
    "                    Try to reduce to at maximum <%s> jobs\")%(int(multiprocessing.cpu_count())-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temporary directories for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_tempdirs(list_of_directories):\n",
    "    '''\n",
    "    Function that create needed temporary folder. Those name have to be saved as other function will depend of the name of those folder.\n",
    "    '''\n",
    "    return_list = []\n",
    "    tmp_grass_dir=gscript.tempdir()\n",
    "    \n",
    "    for directory in list_of_directories:    \n",
    "        # Temporary directory for administrative units statistics\n",
    "        outputdirectory=os.path.join(tmp_grass_dir,directory)\n",
    "        if not os.path.exists(outputdirectory):\n",
    "            os.makedirs(outputdirectory)\n",
    "        return_list.append(outputdirectory)\n",
    "    # Return paths\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary folder and get their paths\n",
    "outputdirectory_admin, outputdirectory_grid = create_tempdirs([\"admin_level\", \"grid_level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a clumped grid for dasymetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This grid will be the reference grid for re-allocation of population count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clumped grid\n",
    "create_clumped_grid(tile_size=tile_size, mask_raster=Land_cover, output='clumped_grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create layers with grid boundary for both levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridded_admin_boundaries(input_vector, id, pop_column, grid):\n",
    "    '''\n",
    "    Function convecting the vector to raster then raster to vector: boundaries will have a \"staircase\" appearence\n",
    "    so that each tile of the gridded vector will be contained in only one administrative unit\n",
    "    '''\n",
    "    \n",
    "    def check_no_missing_zones(vector_origin, vector_gridded, resolution):\n",
    "        '''\n",
    "        Function checking if the number of items (admin zones) in the original vector provided by the user is wall conserved after the rasterization.\n",
    "        If the original vector contains small sized polygons (or very tight) and desired 'tile_size' is too large, some polygons could disappeared during the rasterization process\n",
    "        '''\n",
    "        origin_n=gscript.parse_command('v.db.univar', flags='g', map=vector_origin, column='cat')['n']\n",
    "        gridded_n=gscript.parse_command('v.db.univar', flags='g', map=vector_gridded, column='cat')['n']\n",
    "        if origin_n != gridded_n:\n",
    "            gscript.run_command('g.remove', quiet=True, type='vector', name=vector_gridded, flags='fb')\n",
    "            message=_((\"A tile size of %s m seems too large and produce loss of some polygons when rasterizing them.\\n\") % resolution)\n",
    "            message+=_((\"Try to reduce the 'tile_size' parameter or edit the <%s> vector to merge smallest administrative units with their neighoring units\") % vector_origin)\n",
    "            gscript.fatal(message)\n",
    "        \n",
    "    current_mapset = gscript.gisenv()['MAPSET']\n",
    "    gscript.run_command('g.region', raster=grid)\n",
    "    resolution = int(float(gscript.parse_command('g.region', flags='pg')['nsres']))\n",
    "    global gridded_admin_units\n",
    "    gridded_admin_units = random_layer_name(prefix='gridded_admin_units')\n",
    "    gridded_vector = input_vector.split(\"@\")[0]+'_'+str(resolution)+'m_gridded'+'@'+current_mapset\n",
    "    gscript.run_command('v.to.rast', quiet=True, input=input_vector, type='area', \n",
    "                        output=gridded_admin_units, use='attr', \n",
    "                        attribute_column=id, overwrite=True)\n",
    "    gscript.run_command('r.to.vect', quiet=True, input=gridded_admin_units, \n",
    "                        output=gridded_vector, type='area', column=id, \n",
    "                        flags='v',overwrite=True)\n",
    "    tmp_name=random_layer_name()\n",
    "    gscript.run_command('g.copy', quiet=True, vector='%s,%s'%(input_vector,tmp_name))\n",
    "    gscript.run_command('v.db.join', quiet=True, map_=gridded_vector, column='cat', other_table=tmp_name, other_column=id, subset_columns=pop_column) #join the population count\n",
    "    gscript.run_command('g.remove', quiet=True, flags='f', type='vector', name=tmp_name+'@'+current_mapset)\n",
    "    check_no_missing_zones(input_vector, gridded_vector, resolution)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer with boundaries corresponding to the grid \n",
    "gridded_admin_boundaries(\"admin_level0\", 'cluster', population_column, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of values from categorical raster (land cover and land use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Data_prep(categorical_raster):\n",
    "    '''\n",
    "    Function that extracts resolution and sorted list of classes of \n",
    "    a categorical raster (like land cover or land use information).\n",
    "    '''\n",
    "    info = gscript.raster_info(categorical_raster)\n",
    "    nsres=info.nsres\n",
    "    ewres=info.ewres\n",
    "    L = []\n",
    "    L=[cl.split(\"\t\")[0] for cl in gscript.parse_command('r.category',map=categorical_raster)]\n",
    "    for i,x in enumerate(L):  #Make sure the format is UTF8 and not Unicode\n",
    "        L[i]=x.encode('UTF8')\n",
    "    L.sort(key=float) #Sort the raster categories in ascending.\n",
    "    return nsres, ewres, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of raster 'landcover' used: 10,22,23,33,34,45,111,112,113\n"
     ]
    }
   ],
   "source": [
    "# Data preparation : extract list of classes from the Land Cover\n",
    "lc_classes_list = Data_prep(Land_cover)[2]\n",
    "message=\"Classes of raster '\"+str(Land_cover)+\"' used: \"+\",\".join(lc_classes_list)\n",
    "log_text+=message+'\\n'\n",
    "print message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of raster 'landuse' used: 1,2,3,4,5,6,7,8\n"
     ]
    }
   ],
   "source": [
    "# Data preparation : extract list of classes from the land use\n",
    "lu_classes_list = Data_prep(Land_use)[2]\n",
    "message=\"Classes of raster '\"+str(Land_use)+\"' used: \"+\",\".join(lu_classes_list)\n",
    "log_text+=message+'\\n'\n",
    "print message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute proportion of each land cover and land use class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_class(rasterLayer, cl):\n",
    "    '''\n",
    "    Function extracting a binary map for class 'cl' in raster 'rasterLayer', then computing the proportion of this class in both administratives units and in grids.\n",
    "    The computational region should be defined properly before running this function.\n",
    "    '''\n",
    "    #global outputdirectory_admin, outputdirectory_grid\n",
    "    #Set the region to match the extend of the raster\n",
    "    ### Create a binary raster for the current class\n",
    "    if rasterLayer == Land_cover.split(\"@\")[0]:\n",
    "        prefix = 'LC' \n",
    "    elif rasterLayer == Land_use.split(\"@\")[0]:\n",
    "        prefix = 'LU' \n",
    "    else: prefix = 'MR'  \n",
    "    # Adaptative prefix according to the input raster (land_cover of land_use)\n",
    "    binary_raster = prefix+\"_\"+cl  # Set the name of the binary raster\n",
    "    gscript.run_command('r.mapcalc', expression='%s=if(%s==%s,1,0)'%(binary_raster,rasterLayer,cl),\n",
    "                        overwrite=True,quiet=True) # Mapcalc to create binary raster for the expected class 'cl'\n",
    "    ### Create a temporary copy of the current binary raster with all pixels values equal to 1 (to be used for computing proportion of current binary class)\n",
    "    tmplayer = random_layer_name(prefix='tmp_%s'%binary_raster)\n",
    "    gscript.run_command('r.mapcalc', expression='%s=if(%s==1,1,1)'%(tmplayer,binary_raster),\n",
    "                        overwrite=True,quiet=True)\n",
    "    # Fill potential remaining null values with 0 value (when using r.mapcalc, null values existing in the 'rasterLayer' will remain null in the binary)\n",
    "    gscript.run_command('r.null', quiet=True, map=binary_raster, null='0')\n",
    "    gscript.run_command('r.null', quiet=True, map=tmplayer, null='0')\n",
    "    \n",
    "    ### Compute proportion of pixels of the current class - Administrative units\n",
    "    stat_csv=os.path.join(outputdirectory_admin,\"%s_%s.csv\"%(prefix,cl))\n",
    "    ref_map = gridded_admin_units\n",
    "    gscript.run_command('i.segment.stats', flags='s', map=ref_map, rasters='%s,%s'%(tmplayer,binary_raster), raster_statistics='sum', csvfile=stat_csv, separator='comma', quiet=True, overwrite=True)\n",
    "    output_csv_1=compute_proportion_csv(stat_csv) #Create a new csv containing the proportion\n",
    "    ### Compute proportion of pixels of the current class - Grids\n",
    "    stat_csv=os.path.join(outputdirectory_grid,\"%s_%s.csv\"%(prefix,cl))\n",
    "    ref_map='clumped_grid'\n",
    "    gscript.run_command('i.segment.stats', flags='s', map=ref_map, rasters='%s,%s'%(tmplayer,binary_raster), raster_statistics='sum', csvfile=stat_csv, separator='comma', quiet=True, overwrite=True)\n",
    "    output_csv_2=compute_proportion_csv(stat_csv) #Create a new csv containing the proportion\n",
    "    \n",
    "    ### Remove temporary layer\n",
    "    gscript.run_command('g.remove', quiet=True, flags='f',type='raster',name=tmplayer)\n",
    "    # Return lists\n",
    "    return (binary_raster,output_csv_1,output_csv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def compute_proportion_csv(infile):\n",
    "    '''\n",
    "    Function used in 'proportion_class' function. It take as input the csv from i.segment.stats with the area (in number of pixels)\n",
    "    the sum of pixels of the binary raster and create a new csv with the proportion\n",
    "    '''\n",
    "    # Set the path to the outputfile\n",
    "    head, tail = os.path.split(infile)\n",
    "    root, ext = os.path.splitext(tail)\n",
    "    outfile=os.path.join(head,root+\"_prop\"+ext)\n",
    "    # Create new csv reader and writer objects\n",
    "    reader=csv.reader(open(infile,'r'), delimiter=\",\")\n",
    "    writer=csv.writer(open(outfile,'w'), delimiter=\",\")\n",
    "    # Initialize empty lists\n",
    "    crash_report=[]\n",
    "    content=[]\n",
    "    # Save the first line as header and create the new header\n",
    "    header=reader.next()\n",
    "    new_header=[]\n",
    "    new_header.append(header[0])\n",
    "    index=header[2].find(\"_sum\")\n",
    "    new_header.append(header[2][:index]+'_proportion')\n",
    "    content.append(new_header)  #Create new header with first original column and current class related name for proportion\n",
    "    # Loop through the rest of the rows (header is passed)\n",
    "    for row in reader:\n",
    "        pix_nb=float(row[1]) #Area of the unit (in number of pixels)\n",
    "        class_nb=float(row[2]) #Number of pixels of current class (binary raster)\n",
    "        try:\n",
    "            prop=100*class_nb/pix_nb\n",
    "            content.append([row[0],\"{0:.5f}\".format(prop)])\n",
    "        except ZeroDivisionError:  #If computation of proportion failed because of 'ZeroDivisionError'\n",
    "            crash_report.append(row[0])\n",
    "            content.append([row[0],\"{0:.5f}\".format(0.0)])  # If ZeroDivisionError, set the proportion to zero to avoid errors in next steps\n",
    "            continue\n",
    "    writer.writerows(content)\n",
    "    os.remove(infile)\n",
    "    # Print notification of ZeroDivisionError if it happened\n",
    "    if len(crash_report)>0:\n",
    "        print \"An 'ZeroDivisionError' has been registered for the following <%s>\"%header[0]+\"\\n\".join(crash_report)\n",
    "    # Return the path to the temporary csv file\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VHR Land cover proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proportion of each class of categorical raster computed in: 24.7 seconds'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save time at starting\n",
    "start = start_processing() \n",
    "## Compute proportion of each class of categorical raster (parallel processing).\n",
    "#gscript.run_command('g.region', raster=Land_cover.split(\"@\")[0])  #Set the region to match the extend of the raster\n",
    "p=Pool(n_jobs) #Create a 'pool' of processes and launch them using 'map' function\n",
    "func=partial(proportion_class,Land_cover.split(\"@\")[0]) # Set fixed argument of the function\n",
    "output=p.map(func,lc_classes_list) # Launch the processes for as many items in the list (if function with a return, the returned results are ordered thanks to 'map' function)\n",
    "p.close()\n",
    "p.join()\n",
    "temp_rasterlist,temp_csvlist_1,temp_csvlist_2=zip(*output)\n",
    "[TMP_MAPS.append(x) for x in temp_rasterlist]  # Append the name of binary rasters to the list of temporary maps\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_1]  # Append the paths to .csv files to the list of temporary .csv\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_2]  # Append the paths to .csv files to the list of temporary .csv\n",
    "# Print processing time\n",
    "print_processing_time(begintime=start, printmessage=\"Proportion of each class of categorical raster computed in: \")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VHR Land use proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proportion of each class of categorical raster computed in: 22.4 seconds'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save time at starting\n",
    "start = start_processing() \n",
    "## Compute proportion of each class of categorical raster (parallel processing).\n",
    "#gscript.run_command('g.region', raster=Land_use.split(\"@\")[0])  #Set the region to match the extend of the raster\n",
    "p=Pool(n_jobs) #Create a 'pool' of processes and launch them using 'map' function\n",
    "func=partial(proportion_class,Land_use.split(\"@\")[0]) # Set fixed argument of the function\n",
    "output=p.map(func,lu_classes_list) # Launch the processes for as many items in the list (if function with a return, the returned results are ordered thanks to 'map' function)\n",
    "p.close()\n",
    "p.join()\n",
    "temp_rasterlist,temp_csvlist_1,temp_csvlist_2=zip(*output)\n",
    "[TMP_MAPS.append(x) for x in temp_rasterlist]  # Append the name of binary rasters to the list of temporary maps\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_1]  # Append the paths to .csv files to the list of temporary .csv\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_2]  # Append the paths to .csv files to the list of temporary .csv\n",
    "# Print processing time\n",
    "print_processing_time(begintime=start, printmessage=\"Proportion of each class of categorical raster computed in: \")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MR Built-up proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proportion of MR Built-up raster computed in: 20.8 seconds'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save time at starting\n",
    "start = start_processing() \n",
    "## Compute proportion of each class of categorical raster (parallel processing).\n",
    "#gscript.run_command('g.region', raster=mr_builtup.split(\"@\")[0])  #Set the region to match the extend of the raster\n",
    "temp_rasterlist,temp_csvlist_1,temp_csvlist_2 = proportion_class(mr_builtup.split(\"@\")[0],mr_built_pixelvalue)\n",
    "[TMP_MAPS.append(x) for x in temp_rasterlist]  # Append the name of binary rasters to the list of temporary maps\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_1]  # Append the paths to .csv files to the list of temporary .csv\n",
    "[TMP_CSV.append(x) for x in temp_csvlist_2]  # Append the paths to .csv files to the list of temporary .csv\n",
    "# Print processing time\n",
    "print_processing_time(begintime=start, printmessage=\"Proportion of MR Built-up raster computed in: \")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all .csv files with statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os \n",
    "\n",
    "def atoi(text):\n",
    "    '''\n",
    "    Function that return integer if text is digit - Used in 'natural_keys' function\n",
    "    '''\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):   #Return key to be used for sorting string containing numerical values - Used in 'join_csv' function\n",
    "    '''\n",
    "    Trick was found here\n",
    "    https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "\n",
    "    '''\n",
    "    import re  #Import needed library\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]  #Split the string\n",
    "\n",
    "\n",
    "def ordered_list_of_path(indir,pattern_A,pattern_B=\"\",pattern_C=\"\"):\n",
    "    '''\n",
    "    Function that return a list of ordered path for the files in the folder 'indir'.\n",
    "    'pattern_A', 'pattern_B', 'pattern_C'\n",
    "    '''\n",
    "    # Make a list of .csv files according to their filename pattern\n",
    "    os.chdir(indir) # Change the current directory to the folder containing all the .csv files\n",
    "    csvList=glob.glob(pattern_A) #Make a list of strings with the name of .csv files\n",
    "    csvList.sort(key=natural_keys) #Sort the list on a human natural order (for strings containing numericals)\n",
    "    if pattern_B !=\"\":\n",
    "        csvList_B=glob.glob(pattern_B) #Make a list of strings with the name of .csv files\n",
    "        csvList_B.sort(key=natural_keys) #Sort the list on a human natural order (for strings containing numericals)\n",
    "        for item in csvList_B:\n",
    "            csvList.append(item)\n",
    "    if pattern_C !=\"\":\n",
    "        csvList_C=glob.glob(pattern_C) #Make a list of strings with the name of .csv files\n",
    "        csvList_C.sort(key=natural_keys) #Sort the list on a human natural order (for strings containing numericals)\n",
    "        for item in csvList_C:\n",
    "            csvList.append(item)\n",
    "    return csvList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_2csv(file1,file2,separator=\";\",join='inner',fillempty='NULL'):\n",
    "    '''\n",
    "    Function that join two csv files according to the first column (primary key).\n",
    "    'file1' and 'file2' wait for complete path (strings) to the corresponding files. Please not that 'file1' is assume to be the left-one in the join\n",
    "    'separator' wait for the character to be considered as .csv delimiter (string)\n",
    "    'join' parameter wait either for 'left' or 'inner' according to type of join\n",
    "    'fillempty' wait for the string to be use to fill the blank when no occurance is found for the join operation\n",
    "    '''\n",
    "    import time,csv,os\n",
    "    header_list=[]\n",
    "    file1_values_dict={}\n",
    "    file2_values_dict={}\n",
    "    reader1=csv.reader(open(file1), delimiter=separator) #Csv reader for file 1\n",
    "    reader2=csv.reader(open(file2), delimiter=separator) #Csv reader for file 2\n",
    "    # Make a list of headers\n",
    "    header_list1=[ x for x in reader1.next()]\n",
    "    header_list2=[ x for x in reader2.next()[1:]]\n",
    "    # Make a list of unique IDs from the first and second table according to type of join\n",
    "    if join=='inner':\n",
    "        id_list=[row[0] for row in reader1]\n",
    "        [id_list.append(row[0]) for row in reader2]\n",
    "        id_list=list(set(id_list))\n",
    "        id_list.sort(key=natural_keys)\n",
    "    if join=='left':\n",
    "        id_list=[row[0] for row in reader1]\n",
    "        id_list=list(set(id_list))\n",
    "        id_list.sort(key=natural_keys)\n",
    "    # Build dictionnary for values of file 1\n",
    "    reader1=csv.reader(open(file1), delimiter=separator)\n",
    "    reader1.next()\n",
    "    values_dict1={rows[0]:rows[1:] for rows in reader1}\n",
    "    # Build dictionnary for values of file 2\n",
    "    reader2=csv.reader(open(file2), delimiter=separator)\n",
    "    reader2.next()\n",
    "    values_dict2={rows[0]:rows[1:] for rows in reader2}\n",
    "    # Built new content\n",
    "    new_content=[]\n",
    "    new_header=header_list1+header_list2\n",
    "    new_content.append(new_header)\n",
    "    for key in id_list:\n",
    "        new_row=[key]\n",
    "        try:\n",
    "            [new_row.append(value) for value in values_dict1[key]]\n",
    "        except:\n",
    "            [new_row.append('%s'%fillempty) for x in header_list1[1:]]\n",
    "        try:\n",
    "            [new_row.append(value) for value in values_dict2[key]]\n",
    "        except:\n",
    "            [new_row.append('%s'%fillempty) for x in header_list2]\n",
    "        new_content.append(new_row)\n",
    "    #Return the result\n",
    "    outfile=gscript.tempfile()\n",
    "    fout=open(outfile,\"w\")\n",
    "    writer=csv.writer(fout, delimiter=separator)\n",
    "    writer.writerows(new_content) #Write multiples rows in the file\n",
    "    time.sleep(0.5) # To be sure the file will not be close to fast (the content could be uncompletly filled)\n",
    "    fout.close()\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_multiplecsv(fileList,outfile,separator=\";\",join='inner', fillempty='NULL', overwrite=False):\n",
    "    '''\n",
    "    Function that apply join on multiple csv files\n",
    "    '''\n",
    "    import os, sys, shutil\n",
    "    # Stop execution if outputfile exitst and can not be overwriten\n",
    "    if os.path.isfile(outfile) and overwrite==False:\n",
    "        print \"File '%s' aleady exists and overwrite option is not enabled.\"%outfile\n",
    "    else:\n",
    "        if os.path.isfile(outfile) and overwrite==True:  # If outputfile exitst and can be overwriten\n",
    "            os.remove(outfile)\n",
    "            #print \"File '%s' will be overwrited.\"%outfile   # Uncomment if you want a print\n",
    "        nbfile=len(fileList)\n",
    "        if nbfile<=1: #Check if there are at least 2 files in the list\n",
    "            sys.exit(\"This function require at least two .csv files to be jointed together.\")\n",
    "        # Copy the list of file in a queue list\n",
    "        queue_list=list(fileList)\n",
    "        # Left join on the two first files\n",
    "        file1=queue_list.pop(0)\n",
    "        file2=queue_list.pop(0)\n",
    "        tmp_file=join_2csv(file1,file2,separator=separator,join=join, fillempty=fillempty)\n",
    "        # Left join on the rest of the files in the list\n",
    "        while len(queue_list)>0:\n",
    "            file2=queue_list.pop(0)\n",
    "            tmp_file=join_2csv(tmp_file,file2,separator=separator,join=join, fillempty=fillempty)\n",
    "        #Copy the temporary file to the desired output path\n",
    "        shutil.copy2(tmp_file,outfile)\n",
    "        # Print what happend\n",
    "        #print \"%s individual .csv files were joint together.\"%nbfile    # Uncomment if you want a print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Join .csv files of statistics\n",
    "for directory in [outputdirectory_grid, outputdirectory_admin]:\n",
    "    allstatfile=os.path.join(directory,\"all_stats.csv\")\n",
    "    pattern_A=\"LC_*_prop.csv\"   #Add all csv with proportions of Land cover classes\n",
    "    pattern_B=\"LU_*_prop.csv\"  #Add all csv with proportions of Land use classes\n",
    "    pattern_C=\"MR_*_prop.csv\" \n",
    "    list_paths = ordered_list_of_path(directory,pattern_A,pattern_B,pattern_C)\n",
    "    join_multiplecsv(list_paths,allstatfile,separator=\",\",join='inner', fillempty='NULL', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    ## Random Forest\n",
    "    RandomForest(output_weighting_layer,vector.split(\"@\")[0],id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r.population.density.py -a -f --overwrite vector=admin_level0@PERMANENT land_cover=landcover@PERMANENT land_use=landuse@PERMANENT tile_size=100 id=Sect_ID population=pop_Total output=Ouaga_lc_lu_100m_weighting_RF plot=/media/tais/My_Book_1/MAUPP/Traitement/Population_modelling_dasymetry/Results/Ouagadougou/RF_feature_importance.png log_file=/media/tais/My_Book_1/MAUPP/Traitement/Population_modelling_dasymetry/Results/Ouagadougou/RF_log n_jobs=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Re-allocate population count from level 0 to the grid level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.area.weigh --overwrite vector=admin_level0_100m_gridded@PERMANENT column=pop_Total weight=Ouaga_lc_lu_100m_weighting_RF@PERMANENT output=Ouaga_lc_lu_100m_pop_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cleaning mapset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove all temporary layers\n",
    "# raster corresponding to administrative units (level0)\n",
    "gscript.run_command('g.remove', quiet=True, flags='f',type='raster',name=gridded_admin_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
